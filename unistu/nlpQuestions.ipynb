{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensevalInstance(word='hard-a', position=20, context=[('``', '``'), ('he', 'PRP'), ('may', 'MD'), ('lose', 'VB'), ('all', 'DT'), ('popular', 'JJ'), ('support', 'NN'), (',', ','), ('but', 'CC'), ('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'), ('kill', 'VB'), ('him', 'PRP'), ('to', 'TO'), ('defeat', 'VB'), ('him', 'PRP'), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('do', 'VB'), ('.', '.'), (\"''\", \"''\")], senses=('HARD1',))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "\n",
    "# Load the serve.pos dataset\n",
    "instances = senseval.instances('hard.pos')\n",
    "\n",
    "# Print the first instance\n",
    "print(instances[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_features(instance):\n",
    "    # Extract the context sentence and target word position\n",
    "    context = instance.context\n",
    "    target_index = instance.position\n",
    "    \n",
    "    # Define the window size for context words\n",
    "    window = 3\n",
    "    \n",
    "    # Extract the context words within the window around the target word\n",
    "    features = {}\n",
    "    for i in range(max(0, target_index - window), target_index):\n",
    "        features[f\"word_before_{i-target_index}\"] = context[i]\n",
    "    for i in range(target_index+1, min(len(context), target_index + window + 1)):\n",
    "        features[f\"word_after_{i-target_index-1}\"] = context[i]\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and labels for each instance\n",
    "featuresets = [(serve_features(inst), inst.senses[0]) for inst in instances]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145496535796767\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     10.4 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      8.7 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.1 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      6.3 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      5.7 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      5.6 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      5.6 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      5.4 : 1.0\n",
      "         contains(flynt) = True              pos : neg    =      5.1 : 1.0\n",
      "    contains(ridiculous) = True              neg : pos    =      5.1 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      4.9 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      4.9 : 1.0\n",
      "          contains(jedi) = True              pos : neg    =      4.8 : 1.0\n",
      "       contains(unfunny) = True              neg : pos    =      4.3 : 1.0\n",
      "         contains(worst) = True              neg : pos    =      4.3 : 1.0\n",
      "         contains(bland) = True              neg : pos    =      4.2 : 1.0\n",
      "        contains(superb) = True              pos : neg    =      4.2 : 1.0\n",
      "           contains(era) = True              pos : neg    =      4.2 : 1.0\n",
      "     contains(fantastic) = True              pos : neg    =      4.1 : 1.0\n",
      "        contains(allows) = True              pos : neg    =      4.0 : 1.0\n",
      "        contains(stupid) = True              neg : pos    =      3.9 : 1.0\n",
      "          contains(dull) = True              neg : pos    =      3.8 : 1.0\n",
      "     contains(laughable) = True              neg : pos    =      3.8 : 1.0\n",
      "          contains(mess) = True              neg : pos    =      3.6 : 1.0\n",
      "     contains(pointless) = True              neg : pos    =      3.6 : 1.0\n",
      "      contains(terrific) = True              pos : neg    =      3.6 : 1.0\n",
      "         contains(badly) = True              neg : pos    =      3.5 : 1.0\n",
      "        contains(boring) = True              neg : pos    =      3.5 : 1.0\n",
      "     contains(portrayal) = True              pos : neg    =      3.5 : 1.0\n",
      "     contains(memorable) = True              pos : neg    =      3.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Define a feature extractor for movie reviews\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Define the 2000 most frequent words as the feature set\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# Extract the document features and split the dataset into training and testing sets\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# Train a Naive Bayes classifier on the training set\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Print the 30 most informative features\n",
    "print(classifier.show_most_informative_features(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
